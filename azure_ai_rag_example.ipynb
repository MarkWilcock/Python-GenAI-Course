{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#import openai\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import AzureOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\") or \"\"\n",
    "    )\n",
    "# This corresponds to the custom name when we deployed a model on the Azure OpenAI service\n",
    "deployment_name='training-gpt35-deployment-001' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "text = extract_text_from_pdf('pdf/Wiltshire Item10a_Appendix 1 - MTFS.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"Split text into smaller chunks.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "splits = split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(chunks, query, top_n=3):\n",
    "    \"\"\"Retrieve the most relevant chunks based on keyword matching.\"\"\"\n",
    "    return sorted(chunks, key=lambda x: query.lower() in x.lower(), reverse=True)[:top_n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell belows shows  exactly how the function above works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = ['aaa', 'bbb', 'ccc', 'ddd', 'eee', 'abc', 'def', 'ghi', 'jkl', 'mno', 'aaz', 'azz']\n",
    "test_result = retrieve_relevant_chunks(my_list, \"a\", top_n=3)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks2(chunks, query, top_n=3):\n",
    "    \"\"\"Retrieve the most relevant chunks based on a count of the number of times the query appears in an element of chunks.\"\"\"\n",
    "    return sorted(chunks, key=lambda chunk: chunk.lower().count(query.lower()), reverse=True)[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function works better since it sort on the countr, not simply the existing of the substring in the chunk\n",
    "test_result = retrieve_relevant_chunks2(my_list, \"a\", top_n=3)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = retrieve_relevant_chunks(splits, \"finance\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunks):\n",
    "    \"\"\"Convert chunks to embeddings and store in a FAISS vector database.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(context, query):\n",
    "    \"\"\" build the RAG prompt from the context and query \"\"\"\n",
    "    return f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "#print(build_prompt(\"context......\", \"questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(context, query):\n",
    "    \"\"\"Generate a response from the prompt.\"\"\"\n",
    "    prompt = build_prompt(context, query)\n",
    "    response = client.completions.create(model=deployment_name, prompt=prompt, max_tokens=1000)\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = ['pdf/' + file for file in os.listdir('pdf')]\n",
    "pdf_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_files = [\n",
    "#     'pdf/derbyshire-dales-medium-term-financial-plan-appendix-1.pdf', \n",
    "#     'pdf/middlesborough-council-medium-term-financial-plan.pdf'\n",
    "# ]\n",
    "\n",
    "# Extract text from PDFs and split into chunks\n",
    "all_chunks = []\n",
    "for pdf_path in pdf_files:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = split_text(text)\n",
    "    all_chunks.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = create_vector_store(all_chunks)\n",
    "vector_store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the  query, find the top n matching chunks and combines these into a single text string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Are the finances of each council on a sound basis?\"\n",
    "\n",
    "search_results = vector_store.similarity_search(query, top_k=10)\n",
    "contents = [result.page_content for result in search_results]\n",
    "context = '\\n\\n'.join(contents)\n",
    "\n",
    "context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAI to generate a response\n",
    "response = generate_response(context, query)\n",
    "print(\"Response:\", response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
