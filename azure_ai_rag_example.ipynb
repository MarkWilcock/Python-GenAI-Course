{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.identity\n",
    "import os\n",
    "import openai\n",
    "import PyPDF2\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from openai import AzureOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markw\\repos\\Python-GenAI-Course\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3489: UserWarning: WARNING! azure_deployment is not default parameter.\n",
      "                azure_deployment was transferred to model_kwargs.\n",
      "                Please confirm that azure_deployment is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "c:\\Users\\markw\\repos\\Python-GenAI-Course\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3489: UserWarning: WARNING! api_version is not default parameter.\n",
      "                api_version was transferred to model_kwargs.\n",
      "                Please confirm that api_version is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "c:\\Users\\markw\\repos\\Python-GenAI-Course\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3489: UserWarning: WARNING! azure_endpoint is not default parameter.\n",
      "                azure_endpoint was transferred to model_kwargs.\n",
      "                Please confirm that azure_endpoint is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Completions.create() got an unexpected keyword argument 'azure_deployment'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     13\u001b[39m messages = [\n\u001b[32m     14\u001b[39m     HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the capital of France?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m ]\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Get the response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m response = \u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Output the content of the assistant's reply\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.content)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\markw\\repos\\Python-GenAI-Course\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:307\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    298\u001b[39m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[32m   (...)\u001b[39m\u001b[32m    302\u001b[39m     **kwargs: Any,\n\u001b[32m    303\u001b[39m ) -> BaseMessage:\n\u001b[32m    304\u001b[39m     config = ensure_config(config)\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    306\u001b[39m         ChatGeneration,\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    317\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\markw\\repos\\Python-GenAI-Course\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:843\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    836\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    837\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[32m   (...)\u001b[39m\u001b[32m    840\u001b[39m     **kwargs: Any,\n\u001b[32m    841\u001b[39m ) -> LLMResult:\n\u001b[32m    842\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\markw\\repos\\Python-GenAI-Course\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:683\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[32m    681\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    682\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m683\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    687\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    689\u001b[39m         )\n\u001b[32m    690\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    691\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\markw\\repos\\Python-GenAI-Course\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:908\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    906\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    907\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    911\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    912\u001b[39m         result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\markw\\repos\\Python-GenAI-Course\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:925\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    923\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\markw\\repos\\Python-GenAI-Course\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Completions.create() got an unexpected keyword argument 'azure_deployment'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Initialize the Azure OpenAI chat client\n",
    "chat = ChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_DEPLOYMENT\"),\n",
    "    api_key=os.getenv(\"AZURE_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_MODEL\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# Example chat input\n",
    "messages = [\n",
    "    HumanMessage(content=\"What's the capital of France?\")\n",
    "]\n",
    "\n",
    "# Get the response\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "# Output the content of the assistant's reply\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_ENDPOINT = os.getenv(\"AZURE_ENDPOINT\")\n",
    "AZURE_KEY = os.getenv(\"AZURE_KEY\")\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint=AZURE_ENDPOINT,\n",
    "    credential=AzureKeyCredential(AZURE_KEY)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_MODEL = os.getenv(\"AZURE_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milan, located in Italy's northern Lombardy region, is renowned for its fashion, design, and rich cultural heritage. There are numerous attractions to see in Milan, and here's a list of some of the most iconic sights:\n",
      "\n",
      "1. **Duomo di Milano (Milan Cathedral)**: The city's most famous landmark. This Gothic cathedral took nearly six centuries to complete and is the fifth-largest cathedral in the world. Be sure to visit the rooftop for a breathtaking view of the city.\n",
      "\n",
      "2. **Galleria Vittorio Emanuele II**: One of the world's oldest shopping malls, boasting a stunning 19th-century glass-vaulted arcade. It's a great place for high-end shopping or simply to enjoy the architecture and perhaps a coffee at one of the elegant cafes.\n",
      "\n",
      "3. **Santa Maria delle Grazie & The Last Supper**: This church and Dominican convent include the refectory that houses Leonardo da Vinci's \"The Last Supper,\" one of the world's most famous paintings. Remember to book tickets in advance as they sell out quickly.\n",
      "\n",
      "4. **Teatro alla Scala (La Scala)**: One of the most prestigious opera houses in the world. If you can't catch a performance, consider visiting the museum to learn about the theater's history and see a collection of costumes, set designs, and musical instruments.\n",
      "\n",
      "5. **Castello Sforzesco**: A massive Renaissance castle that now houses several of the city's museums and art collections. The castle’s grounds, Parco Sempione, are also ideal for a relaxing stroll or picnic.\n",
      "\n",
      "6. **Pinacoteca di Brera**: An important art gallery with an extensive collection of Italian Renaissance art, including works by Caravaggio, Raphael, and Mantegna.\n",
      "\n",
      "7. **San Siro Stadium**: A must-visit for football fans, this stadium is home to AC Milan and Inter Milan, two of Italy’s biggest football clubs.\n",
      "\n",
      "8. **Fashion District (Quadrilatero della Moda)**: For fashion enthusiasts, the streets of Via Montenapoleone and Via della Spiga are filled with luxury boutiques and showrooms.\n",
      "\n",
      "9. **Navigli District**: This area is known for its canals, designed by Leonardo da Vinci, which used to serve the city’s transport system. It's now a trendy neighborhood with lots of cafes, restaurants, and vintage shops, especially lively during the evening aperitivo time.\n",
      "\n",
      "10. **Museum of Science and Technology \"Leonardo da Vinci\"**: An interactive museum that's great for families, covering topics from energy to materials science, and including a display of da Vinci’s engineering inventions.\n",
      "\n",
      "11. **Piazza Gae Aulenti**: A modern piazza surrounded by futuristic skyscrapers and is part of Milan's new skyline.\n",
      "\n",
      "These highlights just skim the surface of what Milan has to offer. Remember to explore the city's culinary scene, which extends far beyond pizza and pasta, and includes a variety of regional specialities and fine wines. Enjoy your trip!\n"
     ]
    }
   ],
   "source": [
    "response = client.complete(\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        UserMessage(content=\"I am going to Milan, what should I see?\")\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=AZURE_MODEL\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whiskers quiver, wait— 🐱\n",
      "Tuna scent fills the air, mew!\n",
      "Feast for feline plate. 🍣\n"
     ]
    }
   ],
   "source": [
    "response = client.complete(\n",
    "    model=AZURE_MODEL,\n",
    "    temperature=0.7,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that makes lots of cat references and uses emojis.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a haiku about a hungry cat who wants tuna\"},\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "text = extract_text_from_pdf('pdf/Wiltshire Item10a_Appendix 1 - MTFS.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"Split text into smaller chunks.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "splits = split_text(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(chunks, query, top_n=3):\n",
    "    \"\"\"Retrieve the most relevant chunks based on keyword matching.\"\"\"\n",
    "    return sorted(chunks, key=lambda x: query.lower() in x.lower(), reverse=True)[:top_n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell belows shows  exactly how the function above works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa', 'abc', 'aaz']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list = ['aaa', 'bbb', 'ccc', 'ddd', 'eee', 'abc', 'def', 'ghi', 'jkl', 'mno', 'aaz', 'azz']\n",
    "test_result = retrieve_relevant_chunks(my_list, \"a\", top_n=3)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks2(chunks, query, top_n=3):\n",
    "    \"\"\"Retrieve the most relevant chunks based on a count of the number of times the query appears in an element of chunks.\"\"\"\n",
    "    return sorted(chunks, key=lambda chunk: chunk.lower().count(query.lower()), reverse=True)[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aaa', 'aaz', 'abc']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function works better since it sort on the countr, not simply the existing of the substring in the chunk\n",
    "test_result = retrieve_relevant_chunks2(my_list, \"a\", top_n=3)\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['17 November 2022  and outlined a n increase in Department Expenditure Limit for Local \\nGovernment of 33% . \\nOn the 19 December 2022 the Secretary of State announced the provisional local \\ngovernment settlement which provided details on the grant allocations to Councils \\nfrom this additional funding, together with other measures aimed at supporting Council \\nfinances.   The provisional settlement set out that Local Government Core Spending',\n",
       " 'SAVINGS  \\nThe Cabinet have been working on putting together saving proposals over the three \\nyears that would not only still enable business plan priorities to be delivered but also \\nfor the Councils finances to be managed and move to a sustainable footing.  \\nSaving proposals have been put forward that total £ 51m over the MTFS, significantly \\nover £2 6m to be delivered in the first year.  \\nThe detail proposal for savings by each service are shown  in annex 7 of this appendix.',\n",
       " 'Corporate Director People Total 236.926 8.790 - 0.972 3.735 0.493 0.500 16.209 11.947 (8.896) 270.676 33.750\\nFinance 2.727 1.271 - 0.587 0.337 - - 0.070 - (0.566) 4.426 1.699\\nAssets & Commercial Development 15.489 1.413 - - 0.211 - - 2.458 0.299 (1.931) 17.939 2.450\\nInformation Services 11.419 0.452 - - 0.229 - - 0.299 0.039 (0.629) 11.809 0.390\\nProcurement & Commissioning 4.976 0.222 - - 0.294 - - - 0.050 (0.415) 5.127 0.151']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = retrieve_relevant_chunks(splits, \"finance\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(chunks):\n",
    "    \"\"\"Convert chunks to embeddings and store in a FAISS vector database.\"\"\"\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        azure_endpoint=os.getenv(\"AZURE_ENDPOINT\"),\n",
    "        #deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "        api_key=os.getenv(\"AZURE_KEY\"),\n",
    "        api_version=os.getenv(\"AZURE_MODEL\")\n",
    "    )\n",
    "\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:\n",
      "context......\n",
      "\n",
      "Question: questions\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "def build_prompt(context, query):\n",
    "    \"\"\" build the RAG prompt from the context and query \"\"\"\n",
    "    return f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "\n",
    "print(build_prompt(\"context......\", \"questions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(context, query):\n",
    "    \"\"\"Generate a response from the prompt.\"\"\"\n",
    "    prompt = build_prompt(context, query)\n",
    "    response = client.completions.create(model=AZURE_MODEL, prompt=prompt, max_tokens=1000)\n",
    "    return response.choices[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pdf/Cambridge Current-Budget-and-MTFP.pdf',\n",
       " 'pdf/derbyshire-dales-medium-term-financial-plan-appendix-1.pdf',\n",
       " 'pdf/middlesborough-council-medium-term-financial-plan.pdf',\n",
       " 'pdf/North Yorkshire 2023-24-Budget-and-Capital-Programme-and-MTFP-to-2026-27-Police.pdf',\n",
       " 'pdf/Wiltshire Item10a_Appendix 1 - MTFS.pdf']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_files = ['pdf/' + file for file in os.listdir('pdf')]\n",
    "pdf_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_files = [\n",
    "#     'pdf/derbyshire-dales-medium-term-financial-plan-appendix-1.pdf', \n",
    "#     'pdf/middlesborough-council-medium-term-financial-plan.pdf'\n",
    "# ]\n",
    "\n",
    "# Extract text from PDFs and split into chunks\n",
    "all_chunks = []\n",
    "for pdf_path in pdf_files:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = split_text(text)\n",
    "    all_chunks.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vector_store = \u001b[43mcreate_vector_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_chunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcreate_vector_store\u001b[39m\u001b[34m(chunks)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_vector_store\u001b[39m(chunks):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Convert chunks to embeddings and store in a FAISS vector database.\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     embeddings = \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     embeddings = AzureOpenAIEmbeddings(\n\u001b[32m      6\u001b[39m         azure_endpoint=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mAZURE_ENDPOINT\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      7\u001b[39m         \u001b[38;5;66;03m#deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\u001b[39;00m\n\u001b[32m      8\u001b[39m         api_key=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mAZURE_KEY\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      9\u001b[39m         api_version=os.getenv(\u001b[33m\"\u001b[39m\u001b[33mAZURE_MODEL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m     )\n\u001b[32m     12\u001b[39m     vector_store = FAISS.from_texts(chunks, embeddings)\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\markw\\repos\\Python-GenAI-Course\\.venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:338\u001b[39m, in \u001b[36mOpenAIEmbeddings.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    336\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_client = httpx.Client(proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy)\n\u001b[32m    337\u001b[39m     sync_specific = {\u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_client}\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[38;5;28mself\u001b[39m.client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msync_specific\u001b[49m\u001b[43m)\u001b[49m.embeddings  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.async_client:\n\u001b[32m    340\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.openai_proxy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.http_async_client:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\markw\\repos\\Python-GenAI-Course\\.venv\\Lib\\site-packages\\openai\\_client.py:114\u001b[39m, in \u001b[36mOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    112\u001b[39m     api_key = os.environ.get(\u001b[33m\"\u001b[39m\u001b[33mOPENAI_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m     )\n\u001b[32m    117\u001b[39m \u001b[38;5;28mself\u001b[39m.api_key = api_key\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mOpenAIError\u001b[39m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "vector_store = create_vector_store(all_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the  query, find the top n matching chunks and combines these into a single text string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mAre the finances of each council on a sound basis?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m search_results = \u001b[43mvector_store\u001b[49m.similarity_search(query, top_k=\u001b[32m10\u001b[39m)\n\u001b[32m      4\u001b[39m contents = [result.page_content \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m search_results]\n\u001b[32m      5\u001b[39m context = \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m.join(contents)\n",
      "\u001b[31mNameError\u001b[39m: name 'vector_store' is not defined"
     ]
    }
   ],
   "source": [
    "query = \"Are the finances of each council on a sound basis?\"\n",
    "\n",
    "search_results = vector_store.similarity_search(query, top_k=10)\n",
    "contents = [result.page_content for result in search_results]\n",
    "context = '\\n\\n'.join(contents)\n",
    "\n",
    "context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAI to generate a response\n",
    "response = generate_response(context, query)\n",
    "print(\"Response:\", response)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
